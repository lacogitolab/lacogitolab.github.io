---
layout: post
title: "Scaling of Neurosymbolic AI"
date: 2025-11-15
author: "Xiaming Chen"
header-img: "img/post-bg-universe.jpg"
tags: ["Neuro-symbolic", "Data efficiency", "Scaling computation"]
---

Artificial neural networks (ANN), especially deep learning, illustrate the enormous
capability of modeling complex patterns in an end-to-end manner. Its iconic
architecture — the Transformer — has extended deep learning applications from
narrow academic domains to pervasive practical fields, both commmercial and
scientific. This success is not only attributed to internet-scale training data
and commercially poured computing resources, but also to the efficiently and
deeply scalable design of the Transformer’s attention architecture. Three
scaling factors — data, computing, and architecture — together establish the
foundational pillars of deep learning’s success.

However, there is a transparent wall that blocks the march of deep learning
toward the so-called singularity of artificial intelligence — or whatever
buzzword people use to fuel their futuristic imagination. This wall is built
from several kinds of bricks: *data depletion*, *weak interpretability*,
*non-explorable reasoning*, *architectural inflexibility*, and *the inability
of continual learning*. A vast disparity remains between the massive capital
invested in neural net development and the modest, incremental gains in
high-level reasoning and real-world applications of critical scenarios.

To overcome the intrinsic limitations of neural networks while still harnessing
the power of scaling, researchers have increasingly converged on combining two
long-standing traditions in AI — symbolic logic and neural networks — giving
rise to what is now known as *neurosymbolic AI*.

## Neurosymbolic Architectures

Kautz (2022) categorized state-of-the-art neurosymbolic architectures into six
types, based on the relative roles of their symbolic and neural modules. Across
the broader landscape of literature, there are certainly variants that differ
from these classifications. Nevertheless, analyzing the categorized
architectures provides a useful foundation for understanding future innovations
that may emerge through the assembly or transformation of known patterns. In
this essay, we focus on examining the scaling mechanisms of these six
neurosymbolic paradigms to inspire the design of noval architectures.

The classified six types of neurosymbolic architectures are illustrated as
below.

![illustration of six types](/)


## Neurosymbolic Scaling


